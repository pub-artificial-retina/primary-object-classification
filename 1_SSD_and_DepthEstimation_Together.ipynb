{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUWdKMmdPc-v"
   },
   "source": [
    "---\n",
    "# **Note(s):**\n",
    "## 1) Metrics to be used for each:\n",
    "* `Image`:\n",
    "  * scene\n",
    "    * estimated based on presumptions, i.e., `scenes`\n",
    "* `Object`:\n",
    "  * area:\n",
    "    * object detected based on single shot detection (SSD) model, i.e., `object_detector`\n",
    "    * calculated based on bound-boxes, i.e., `curr_pred_boxes`\n",
    "  * depth:\n",
    "    * (in progress) predicted based on depth-estimation model, i.e., `depth_estimator`\n",
    "      * got the matrix\n",
    "      * developing code to get distance of each object using the matrix\n",
    "  * **angle**:\n",
    "    * (to be done:) detected/estimated based on object angle-estimation model\n",
    "      * have to find a proper model that does the job ...\n",
    "\n",
    "---\n",
    "# **Xtra(s)**\n",
    "## 1) Future work(s):\n",
    "* improve the image quality in such a way by image-ehancement so that when passed the image to Artificial Retina (AR) chip.\n",
    "* consider minimum stimulation power by involving multi-objects\n",
    "* edge information of primary object(s) (todo)\n",
    "\n",
    "## 2) Question(s):\n",
    "* What is the **frequency** of sending images to the chip?, i.e., The number of images transmitted per time.\n",
    "* `image-size`:\n",
    "  * model-input: `100 x 100`\n",
    "  * model-output: `40 x 40`\n",
    "* `tool` to simulate image to artificial retinal chip, i.e., `Matlab`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg6O7Aw891SK"
   },
   "source": [
    "# **Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5EkCxgbl90iw"
   },
   "outputs": [],
   "source": [
    "# general\n",
    "images_to_consider = 1000\n",
    "\n",
    "# object detection\n",
    "obj_det_probability_threshold = 0.360\n",
    "\n",
    "# distance step factor (multiplicative)\n",
    "obj_multiplicative_step = 1.00015\n",
    "\n",
    "# ignore object(s)\n",
    "obj_ignore_size  = 50 # i.e., 50x50\n",
    "obj_ignore_area = obj_ignore_size*obj_ignore_size\n",
    "obj_ignore_depth = 120 # far\n",
    "obj_ignore_range = 50 # nearest-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cf9sQE9ky9Nb"
   },
   "outputs": [],
   "source": [
    "depth_file_postfix            = \"depth\"\n",
    "object_detection_file_postfix = \"object-detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JacAsXL-UtH"
   },
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i4uUnP3m6luM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 8.12.0\n",
      "ipykernel        : 6.19.2\n",
      "ipywidgets       : 8.0.4\n",
      "jupyter_client   : 8.1.0\n",
      "jupyter_core     : 5.3.0\n",
      "jupyter_server   : 2.5.0\n",
      "jupyterlab       : 3.6.3\n",
      "nbclient         : 0.5.13\n",
      "nbconvert        : 6.5.4\n",
      "nbformat         : 5.7.0\n",
      "notebook         : 6.5.4\n",
      "qtconsole        : 5.4.2\n",
      "traitlets        : 5.7.1\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment in local only\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install for colab only\n",
    "# !pip install transformers==4.30.2\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment in local only\n",
    "# from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg6O7Aw891SK"
   },
   "source": [
    "# **Cuda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "gpus = None\n",
    "if_multi_gpu_which_gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. <torch.cuda.device object at 0x7f04ac54eb00>\n",
      " - name:\t\t\tNVIDIA GeForce RTX 4070 Ti\n",
      " - major/minor:\t\t\t8/9\n",
      " - total_memory:\t\t12878086144 (12.88 GB)\n",
      " - Multi-processor count:\t60\n",
      "\n",
      "*** You are ready to use 'cuda:0' device! ***\n"
     ]
    }
   ],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    device = \"cuda:0\"\n",
    "    gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "    gpus_props = [torch.cuda.get_device_properties(gpu) for gpu in gpus]\n",
    "    for ind, gpu in enumerate(gpus_props):\n",
    "        print (\"{:}. {:}\".format(ind, gpus[ind]))\n",
    "        print (\" - name:\\t\\t\\t{:}\\n - major/minor:\\t\\t\\t{:}/{:}\".format(\n",
    "            gpu.name, gpu.major, gpu.minor,\n",
    "        ))\n",
    "        print (\" - total_memory:\\t\\t{:} ({:.2f} GB)\\n - Multi-processor count:\\t{:}\\n\".format(\n",
    "            gpu.total_memory, (gpu.total_memory/1000000000), gpu.multi_processor_count,\n",
    "        ))\n",
    "        if (if_multi_gpu_which_gpu == ind):\n",
    "            device = \"cuda:{:}\".format(if_multi_gpu_which_gpu)\n",
    "    print (\"*** You are ready to use '{:}' device! ***\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axdb5IXXG1RC"
   },
   "source": [
    "# **Fonts/Colors/Etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TS8sAAlNG0PK"
   },
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "antialiasedline = cv2.LINE_AA\n",
    "\n",
    "boxcolor = (0, 255, 0)\n",
    "textcolor = (255, 255, 0)\n",
    "centercolor = (255, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DP8nx6kVMEf"
   },
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6awq8HbcVLlw"
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UfpRpLsHV0tv"
   },
   "outputs": [],
   "source": [
    "def rectAreaFromCoordinates(x1, y1, x2, y2):\n",
    "  length = x2-x1\n",
    "  height = y2-y1\n",
    "  return length * height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UxoFMORkQT39"
   },
   "outputs": [],
   "source": [
    "def getTagName(tagString):\n",
    "  tmp = tagString.split(\"}\")\n",
    "  tmp_len = len(tmp)\n",
    "  if (tmp_len == 1):\n",
    "    return \"non-tag\"\n",
    "  else:\n",
    "    return tmp[tmp_len-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uaFf4UkSR6QS"
   },
   "outputs": [],
   "source": [
    "def getExtension(nameString):\n",
    "  tmp = nameString.split(\".\")\n",
    "  tmp_len = len(tmp)\n",
    "  if (tmp_len == 1):\n",
    "    return \"non-ext\"\n",
    "  else:\n",
    "    return tmp[tmp_len-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0seuEuu7PQNX"
   },
   "outputs": [],
   "source": [
    "def getDepthAverage(depths, center, layer):\n",
    "  return depths[center[0]][center[1]].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLewz-eg-apS"
   },
   "source": [
    "# **Get the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjMkeUU5fBh7"
   },
   "source": [
    "## **Depth estimation model** (vinvino02/glpn-*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZYWzyT4kcfN"
   },
   "source": [
    "*   GLPN stands for Global-Local Path Networks\n",
    "*   Karlsruhe Institute of Technology and Toyota Technological Institute\n",
    "*   New York University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSO66tJ-uIwG"
   },
   "source": [
    "### **- Depth estimator model 1 (a, b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "039c0b168f1a46fb91674d56bf8048d2",
      "1de0adc0702142bf89e236365f91a84b",
      "ccfe9f7cdd144c84a24ee465fad7e61c",
      "13c9cbab9f7d4be5978be65a98df8f17",
      "f7b3c7a84f504d9e9869131e2983b354",
      "6bc6c52d04254d26ae0456e8ba2c8d12",
      "954a7ca6fd3943e688e68c007346f04e",
      "7a2aa0081611464b8fa74ea418bc5c78",
      "fbfe7cb55fd448a3b64ce92b6b1a1d2d",
      "ef783b28b9444ee9931f08a309fe6817",
      "4555a4b079a8435e99af9ff3297c393f",
      "ee7e1598db4d4caba3841a627f1ff166",
      "b18c7c202c2c44ce94f0a94e8fe963a9",
      "8fa2ec06d6c549a887274db3cacf3ae4",
      "03b868f0f5244e0c973629869bd0f0ae",
      "66691537bd85499a99b71de0b7c5ca98",
      "b3c335b678ef49d1975fa6a9a8674cbe",
      "8c99f279fbc042b8a585ac26161c2ff0",
      "d3a4f74d222147d9bb58cc0e6db2538b",
      "9a29057d635a4687b9ee922fb2bea5e2",
      "940756be252d487b88abd8775c6b7268",
      "925f946a7ea34e1487b2848f53a2bf74",
      "1c75193723c246a89395a596f9f41b78",
      "fa38138784ea4dad8ccfbf63fa772eaf",
      "887294806a7541b9ad1f5eafdc396353",
      "dda27236d3de4dc5ac7e215ba705ec87",
      "3afb0994afda44c081136f2a5c7b7d0a",
      "f52e102e158845b1abb37cdb2477b3d0",
      "1dc9153abb414b53879c699c1deda5e8",
      "db6d1a01c4504af8a5c5938f428c691a",
      "a9a595eb90c4489ebf40e2a4effe666a",
      "3b642cccc8984625b12517535762788e",
      "d9e17a653d7e4f58a7ab7ce08e678cbe"
     ]
    },
    "id": "7OcHw1cafAvO",
    "outputId": "01802568-8324-4753-8344-5be72ee11c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 'depth_estimator' exists!, No need to re-initialize!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_options = [\n",
    "    \"vinvino02/glpn-nyu\",\n",
    "    \"vinvino02/glpn-kitti\",\n",
    "]\n",
    "\n",
    "task = \"depth-estimation\"\n",
    "checkpoint = checkpoint_options[0]\n",
    "\n",
    "# for checking existence in locals() function\n",
    "variable_to_check = \"depth_estimator\"\n",
    "if variable_to_check in locals():\n",
    "  print (\"Variable '{:}' exists!, No need to re-initialize!\".format(variable_to_check))\n",
    "else:\n",
    "  depth_estimator = pipeline(task, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.depth_estimation.DepthEstimationPipeline at 0x7f04abbebfd0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfUTiLV8uodf"
   },
   "source": [
    "### **- Depth estimator model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "40ZMwXtwwdzG"
   },
   "outputs": [],
   "source": [
    "# this will be used to convert the image to tensor. Then this tensor will be used as an input to the depth_estimator model\n",
    "resnet_input_size = (512, 512)\n",
    "preprocess = T.Compose([\n",
    "    T.Resize(resnet_input_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Adjust normalization values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5I7qS-BoRS-",
    "outputId": "5305fce4-a68b-475d-cd98-0ceed35e20f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 'depth_estimator2' exists!, No need to re-initialize!\n"
     ]
    }
   ],
   "source": [
    "# Download the MiDaS model weights\n",
    "checkpoint_url = \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt\"\n",
    "checkpoint_path = \"model_weights.pt\"\n",
    "if (checkpoint_path not in os.listdir()):\n",
    "  torch.hub.download_url_to_file(checkpoint_url, checkpoint_path)\n",
    "\n",
    "# for checking existence in locals() function\n",
    "variable_to_check = \"depth_estimator2\"\n",
    "if variable_to_check in locals():\n",
    "  print (\"Variable '{:}' exists!, No need to re-initialize!\".format(variable_to_check))\n",
    "else:\n",
    "  # Create the MiDaS depth estimation model\n",
    "  depth_estimator2 = torchvision.models.resnet50(pretrained=True)\n",
    "  # depth_estimator2.load_state_dict(torch.load(checkpoint_path))\n",
    "  depth_estimator2 = depth_estimator2.to(device)\n",
    "  # Set the model to evaluation mode\n",
    "  depth_estimator2.eval()\n",
    "  clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xUQwMZfpWg-",
    "outputId": "4c8109ab-c2e6-4398-8a57-30ff33dd6b56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_estimator2.conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfjObvMLfHB5"
   },
   "source": [
    "## **Object detection model** (ssd*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyNzMl0kllI-"
   },
   "source": [
    "* SSD stands for Single Shot Detector\n",
    "* COCO stands for Common Object in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFfNQZhAvifR"
   },
   "source": [
    "### **- Object detector model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35ZsVC1V674I",
    "outputId": "5a425e16-2be5-4eb9-8e1c-91d72ddf4d18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/artificialretina/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/artificialretina/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "object_detector_options = [\n",
    "    \"ssd300_vgg16\",\n",
    "    \"ssdlite320_mobilenet_v3_large\",\n",
    "]\n",
    "\n",
    "object_detector = None\n",
    "object_detector_option = object_detector_options[0]\n",
    "\n",
    "if (object_detector_option == \"ssd300_vgg16\"):\n",
    "  object_detector = torchvision.models.detection.ssd300_vgg16(pretrained = True)\n",
    "elif(object_detector_option == \"ssdlite320_mobilenet_v3_large\"):\n",
    "  object_detector = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = object_detector.to(device)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "SWqiV6Hu7GGS"
   },
   "outputs": [],
   "source": [
    "object_detector.eval()\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5xsa7eUq9GY"
   },
   "source": [
    "### loading coco labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xH9BSZzC_UPX"
   },
   "outputs": [],
   "source": [
    "coco_names =  [\n",
    "    \"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \n",
    "    \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , # 10 \n",
    "    \"fire hydrant\" , \"street sign\" , \"stop sign\" , \"parking meter\" , \"bench\" , \n",
    "    \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , # 20 \n",
    "    \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \n",
    "    \"hat\" , \"backpack\" , \"umbrella\" , \"shoe\" , \"eye glasses\" , # 30 \n",
    "    \"handbag\" , \"tie\" , \"suitcase\" , \"frisbee\" , \"skis\" , \n",
    "    \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" , \"baseball glove\" , # 40 \n",
    "    \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" , \"plate\" , \n",
    "    \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , # 50 \n",
    "    \"bowl\" , \"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \n",
    "    \"broccoli\" , \"carrot\" , \"hot dog\" , \"pizza\" , \"donut\" , # 60 \n",
    "    \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" , \n",
    "    \"mirror\" , \"dining table\" , \"window\" , \"desk\" , \"toilet\" , # 70 \n",
    "    \"door\" , \"tv\" , \"laptop\" , \"mouse\" , \"remote\" , \n",
    "    \"keyboard\" , \"cell phone\" , \"microwave\" , \"oven\" , \"toaster\" , # 80 \n",
    "    \"sink\" , \"refrigerator\" , \"blender\" , \"book\" , \"clock\" , \n",
    "    \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" , # 90 \n",
    "    \"hair brush\", # 91 \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikj1HYRgqmyL"
   },
   "source": [
    "#### trying to get coco_labels from library (instead of hard coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "pqzE-wPdmxyv"
   },
   "outputs": [],
   "source": [
    "# utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "7BWsg0h2mylr"
   },
   "outputs": [],
   "source": [
    "# classes_to_labels = utils.get_coco_object_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "FuPxaqdJn3tE"
   },
   "outputs": [],
   "source": [
    "# print (\"Number of classes (from utils) {:}\".format(len(classes_to_labels)))\n",
    "# print (\"Number of classes (manually) {:}\".format(len(coco_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1FDrm2x_Okz"
   },
   "source": [
    "# **Setting up directories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSV_h5DA8L_D"
   },
   "source": [
    "## clean directories if exist already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Local system\n",
      "- images in 'images'\n",
      "- dataset in 'dataset'\n"
     ]
    }
   ],
   "source": [
    "main_folder = \"\"\n",
    "dataset_folder = \"\"\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "  print (\"Running in Colab\")\n",
    "  main_folder = \"/content/images\"\n",
    "  dataset_folder = \"/content/dataset\"\n",
    "else:\n",
    "  print (\"Running in Local system\")\n",
    "  main_folder = \"images\"\n",
    "  dataset_folder = \"dataset\"\n",
    "print (\"- images in '{:}'\\n- dataset in '{:}'\".format(main_folder, dataset_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "TNQGo8kX8LXN"
   },
   "outputs": [],
   "source": [
    "if (os.path.exists(main_folder) == True):\n",
    "  shutil.rmtree(main_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "HEw-7k7OJLaA"
   },
   "outputs": [],
   "source": [
    "if (os.path.exists(dataset_folder) == True):\n",
    "  shutil.rmtree(dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfbsN4xg05DB"
   },
   "source": [
    "## setting up the source of images (to be loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SrewnPIP05LH"
   },
   "outputs": [],
   "source": [
    "image_src = \"http://images.cocodataset.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TBXU-T5n3xwe"
   },
   "outputs": [],
   "source": [
    "response = requests.get(image_src)\n",
    "xml_content = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ezp8b9dm31aI"
   },
   "outputs": [],
   "source": [
    "tree = ET.fromstring(xml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "moC2HiLOJwIE"
   },
   "outputs": [],
   "source": [
    "root = tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trJ5HCiG4VbC",
    "outputId": "4e55d59a-7b26-4655-c63a-f92487b60702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# of 'Others' tags: 5\n",
      "# of 'Content' tags: 1000\n",
      " - # of 'zip' files: 11\n",
      " - # of 'jpg' files: 988\n",
      " - # of 'non-ext' files: 1\n"
     ]
    }
   ],
   "source": [
    "contents_counter = 0\n",
    "others_counter = 0\n",
    "zip_counter = 0\n",
    "jpg_counter = 0\n",
    "non_counter = 0\n",
    "non_value = \"\"\n",
    "images_dict = {}\n",
    "\n",
    "stop_considering_jpgs = False\n",
    "for content in root:\n",
    "  tagName = getTagName(content.tag)\n",
    "  # print (\"***{:}***:\".format(tagName))\n",
    "  if (tagName == \"Contents\"):\n",
    "    contents_counter += 1\n",
    "    for content2 in content:\n",
    "      tagName2 = getTagName(content2.tag)\n",
    "      # print (\"\\t{:}: '{:}'\".format(tagName2, content2.text))\n",
    "      if (tagName2 == \"Key\"):\n",
    "        fileExtension = getExtension(content2.text)\n",
    "        # print (\"{:}\".format(fileExtension))\n",
    "        if (fileExtension == \"zip\"):\n",
    "          zip_counter += 1\n",
    "        elif (fileExtension == \"jpg\"):\n",
    "          jpg_counter += 1\n",
    "          if (images_to_consider == jpg_counter):\n",
    "            stop_considering_jpgs = True\n",
    "          # print (\"***{:}***:\".format(tagName))\n",
    "          # print (\"\\t{:}: '{:}'\".format(tagName2, content2.text))\n",
    "          # print (\"{:}\".format(fileExtension))\n",
    "          images_dict[jpg_counter-1] = content2.text\n",
    "        elif (fileExtension == \"non-ext\"):\n",
    "          non_counter += 1\n",
    "          non_value = fileExtension\n",
    "    if (stop_considering_jpgs):\n",
    "      break\n",
    "  else:\n",
    "    others_counter += 1\n",
    "  # print (\"\")\n",
    "\n",
    "print (\"\")\n",
    "print (\"# of 'Others' tags: {:}\".format(others_counter))\n",
    "print (\"# of 'Content' tags: {:}\".format(contents_counter))\n",
    "print (\" - # of 'zip' files: {:}\".format(zip_counter))\n",
    "print (\" - # of 'jpg' files: {:}\".format(jpg_counter))\n",
    "print (\" - # of '{:}' files: {:}\".format(non_value, non_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rh4OPTsaKPi",
    "outputId": "0cab5015-15f8-489c-bf40-bc40277113d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'988/1000' number of images will be considered.\n"
     ]
    }
   ],
   "source": [
    "images = len(images_dict)\n",
    "# images = 20\n",
    "print (\"'{:}/{:}' number of images will be considered.\".format(images, images_to_consider))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9Hncl7tayx3"
   },
   "source": [
    "## create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isJ9ybuTaxw3",
    "outputId": "e46b69cf-97db-4af4-f59b-2b8c85c3681c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Folder 'images/987/' created.\n"
     ]
    }
   ],
   "source": [
    "print (\"Setting up (image) directories:\")\n",
    "\n",
    "sub_folder = \"\"\n",
    "for image in range(images):\n",
    "  try:\n",
    "    sub_folder = \"{:}/{:}/\".format(main_folder, image)\n",
    "    os.makedirs(sub_folder)\n",
    "    print (\"- Folder '{:}' created.\".format(sub_folder))\n",
    "  except FileExistsError:\n",
    "    print (\"- Folder '{:}' already exists.\".format(sub_folder))\n",
    "  clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "z--6b_icKcON"
   },
   "outputs": [],
   "source": [
    "img_folder_originals = \"images_originals\"\n",
    "img_folder_depths    = \"images_depths\"\n",
    "img_folder_objects   = \"images_objects\"\n",
    "txt_folder_objects   = \"texts_objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVZaZg6lJxm3",
    "outputId": "b27510b2-3b0b-4e76-c533-76a134da1590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up (dataset) directories:\n",
      "- Folder 'dataset' created.\n",
      "- Folder 'dataset/images_originals/' created.\n",
      "- Folder 'dataset/images_depths/' created.\n",
      "- Folder 'dataset/images_objects/' created.\n",
      "- Folder 'dataset/texts_objects/' created.\n"
     ]
    }
   ],
   "source": [
    "print (\"Setting up (dataset) directories:\")\n",
    "\n",
    "try:\n",
    "  sub_folder = \"{:}\".format(dataset_folder)\n",
    "  os.makedirs(sub_folder)\n",
    "  print (\"- Folder '{:}' created.\".format(sub_folder))\n",
    "except FileExistsError:\n",
    "  print (\"- Folder '{:}' already exists.\".format(sub_folder))\n",
    "\n",
    "try:\n",
    "  sub_folder = \"{:}/{:}/\".format(dataset_folder, img_folder_originals)\n",
    "  os.makedirs(sub_folder)\n",
    "  print (\"- Folder '{:}' created.\".format(sub_folder))\n",
    "except FileExistsError:\n",
    "  print (\"- Folder '{:}' already exists.\".format(sub_folder))\n",
    "\n",
    "try:\n",
    "  sub_folder = \"{:}/{:}/\".format(dataset_folder, img_folder_depths)\n",
    "  os.makedirs(sub_folder)\n",
    "  print (\"- Folder '{:}' created.\".format(sub_folder))\n",
    "except FileExistsError:\n",
    "  print (\"- Folder '{:}' already exists.\".format(sub_folder))\n",
    "\n",
    "try:\n",
    "  sub_folder = \"{:}/{:}/\".format(dataset_folder, img_folder_objects)\n",
    "  os.makedirs(sub_folder)\n",
    "  print (\"- Folder '{:}' created.\".format(sub_folder))\n",
    "except FileExistsError:\n",
    "  print (\"- Folder '{:}' already exists.\".format(sub_folder))\n",
    "\n",
    "try:\n",
    "  sub_folder = \"{:}/{:}/\".format(dataset_folder, txt_folder_objects)\n",
    "  os.makedirs(sub_folder)\n",
    "  print (\"- Folder '{:}' created.\".format(sub_folder))\n",
    "except FileExistsError:\n",
    "  print (\"- Folder '{:}' already exists.\".format(sub_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B64SFIW5ZWCs"
   },
   "source": [
    "## loading images in the folder (**or manually upload them and don't run this block of code**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8Ft_4AsZS_e",
    "outputId": "4fed880e-9dc3-4417-ed48-213d2adf3cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459 test-stuff2017/000000013570.jpg\n",
      "- images/459/0.jpg\n",
      "- http://images.cocodataset.org/test-stuff2017/000000013570.jpg\n",
      "459 test-stuff2017/000000013570.jpg\n",
      "- images/459/0.jpg\n",
      "- http://images.cocodataset.org/test-stuff2017/000000013570.jpg\n"
     ]
    }
   ],
   "source": [
    "for image in range(images):\n",
    "  print (image, images_dict[image])\n",
    "  target_img = \"{:}/{:}/{:}.jpg\".format(main_folder, image, 0)\n",
    "  if (len(images_dict[image]) <77):\n",
    "    src_img = \"{:}/{:}\".format(image_src, images_dict[image])\n",
    "  else:\n",
    "    src_img = \"{:}\".format(images_dict[image])\n",
    "  print (\"-\", target_img)\n",
    "  print (\"-\", src_img)\n",
    "  # !wget -O \"/content/images/0/0.jpg\" \"http://images.cocodataset.org/val2017/000000037777.jpg\"\n",
    "  !wget -O {target_img} {src_img}\n",
    "  clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cJXK3ByB_VVe",
    "outputId": "b38592fe-f01e-4b5b-909b-0f5a98d85739"
   },
   "outputs": [],
   "source": [
    "shutil.make_archive(main_folder, 'zip', main_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvHOmx1nYhzh"
   },
   "source": [
    "# **Presumptions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LrzwZgtLa2ui"
   },
   "outputs": [],
   "source": [
    "showrgb = not True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "aOG1OwnXYhRr"
   },
   "outputs": [],
   "source": [
    "# scenes = {\n",
    "#     \"traffic\" : [\n",
    "#         \"truck\", \"car\",\n",
    "#     ], # 0\n",
    "#     \"cows-in-pasture\" : [\n",
    "#         \"cow\",\n",
    "#     ], # 1\n",
    "#     \"elephant-in-zoo\" : [\n",
    "#         \"elephant\",\n",
    "#     ], # 2\n",
    "#     \"self-picture\" : [\n",
    "#         \"person\", \"tie\",\n",
    "#     ], # 3\n",
    "#     \"public-place\": [\n",
    "#         \"person\", \"tie\",\n",
    "#     ], # 4\n",
    "#     \"buses-on-road\" : [\n",
    "#         \"bus\",\n",
    "#     ], # 5\n",
    "#     \"clock-house\" : [\n",
    "#         \"clock\",\n",
    "#     ], # 6\n",
    "#     \"eating-food\" : [\n",
    "#         \"person\", \"donut\",\n",
    "#     ], # 7\n",
    "#     \"open seas and oceans\" : [\n",
    "#         \"boat\",\n",
    "#     ], # 8\n",
    "#     \"workplace\" : [\n",
    "#         \"person\", \"chair\"\n",
    "#     ], # 9\n",
    "\n",
    "#     # \"sports\" : [\n",
    "#     #     \"person\", \"skateboard\",\n",
    "#     # ],\n",
    "#     # \"people-sitting-outside\" : [\n",
    "#     #     \"people\", \"bench\", \"potted plant\",\n",
    "#     # ],\n",
    "#     # \"person-in-tv-lounge\" : [\n",
    "#     #     \"person\", \"tv\", \"couch\",\n",
    "#     # ],\n",
    "#     # \"person-in-bedroom\" : [\n",
    "#     #     \"person\", \"bed\",\n",
    "#     # ],\n",
    "#     # \"fruit-market\" : [\n",
    "#     #     \"person\", \"banana\",\n",
    "#     # ],\n",
    "#     # \"person-dining\" : [\n",
    "#     #     \"person\", \"dining table\", \"chair\", \"table\", \"cup\", \"bowl\", \"teddy bear\",\n",
    "#     # ],\n",
    "\n",
    "#     # \"kitchen\" : [\n",
    "#     #     \"refrigerator\", \"oven\", \"dining table\", \"bowl\",\n",
    "#     # ],\n",
    "# }\n",
    "# default_scene = \"no-scene\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2MFDO3fVGyv"
   },
   "source": [
    "# **Brain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkLzRa0Yp9bY"
   },
   "source": [
    "## Comments to this part:\n",
    "\n",
    "\n",
    "* Scene estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4iTmjBhDBAl"
   },
   "source": [
    "## **Object detection & Depth-estimation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vMdWJym-H6g",
    "outputId": "d9f5f041-ae20-4037-c324-af86af70bdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image # 987:\n",
      " - 0. dining table (1):\n",
      "    - #0 - area:151564, depth:56, score:0.930, (x1, y1): (2, 85), (x2, y2): (367, 499), (xc, yc): (185, 292) - PRIMARY: 1\n",
      " - 1. chair (1):\n",
      "    - #0 - area:10894, depth:152, score:0.890, (x1, y1): (3, 1), (x2, y2): (109, 102), (xc, yc): (56, 52) - PRIMARY: 0\n",
      " - 2. bowl (3):\n",
      "    - #0 - area:17652, depth:45, score:0.770, (x1, y1): (216, 387), (x2, y2): (372, 500), (xc, yc): (294, 444) - PRIMARY: 1\n",
      "    - #1 - area:3274, depth:55, score:0.610, (x1, y1): (5, 236), (x2, y2): (69, 287), (xc, yc): (37, 262) - PRIMARY: 1\n",
      "    - #2 - area:13128, depth:44, score:0.410, (x1, y1): (47, 412), (x2, y2): (196, 500), (xc, yc): (122, 456) - PRIMARY: 1\n"
     ]
    }
   ],
   "source": [
    "transform = T.ToTensor()\n",
    "\n",
    "output = {}\n",
    "\n",
    "# Iterate images\n",
    "for image in range(images):\n",
    "  timestamp = 0\n",
    "  print (\"\")\n",
    "  print (\"Image # {:}:\".format(image))\n",
    "  output[image] = {}\n",
    "  target_img = \"{}/{:}/{:}.jpg\".format(main_folder, image, timestamp)\n",
    "  curr_ig = Image.open(target_img)\n",
    "  curr_img = transform(curr_ig).to(device)\n",
    "  with torch.no_grad():\n",
    "    ##########################\n",
    "    # loading objects detected\n",
    "    curr_pred = object_detector([curr_img])\n",
    "    curr_pred_keys = np.array(list(curr_pred[0].keys()))\n",
    "    curr_pred_num_keys = len(curr_pred_keys)\n",
    "    curr_pred_boxes, curr_pred_scores, curr_pred_labels = curr_pred[0][\"boxes\"], curr_pred[0][\"scores\"], curr_pred[0][\"labels\"]\n",
    "    curr_objects_num = torch.argwhere(curr_pred_scores >= obj_det_probability_threshold).shape[0]\n",
    "    curr_objects = []\n",
    "    curr_igg = cv2.imread(target_img)\n",
    "\n",
    "    ###########################\n",
    "    # loading depth predictions\n",
    "    depth_predictions = depth_estimator(curr_ig)\n",
    "    draw = ImageDraw.Draw(depth_predictions[\"depth\"])\n",
    "\n",
    "    #######################################\n",
    "    # find minimum and maximum pixel values\n",
    "    depth_prediction_min = float('inf')\n",
    "    depth_prediction_max = float('-inf')\n",
    "    depth_prediction_w, depth_prediction_h = depth_predictions[\"depth\"].size\n",
    "    # print (\" - it has '{:}' rows and '{:}' columns.\".format(depth_prediction_w, depth_prediction_h))\n",
    "    for y in range(depth_prediction_h):\n",
    "      for x in range(depth_prediction_w):\n",
    "        depth_prediction_pv = depth_predictions[\"depth\"].getpixel((x, y))\n",
    "        if depth_prediction_pv < depth_prediction_min:\n",
    "          depth_prediction_min = depth_prediction_pv\n",
    "        if depth_prediction_pv > depth_prediction_max:\n",
    "          depth_prediction_max = depth_prediction_pv\n",
    "    # print (\" - the min and max depths of this image is {:}. Diff: {:}\".format(\n",
    "    #     (depth_prediction_min, depth_prediction_max),\n",
    "    #     depth_prediction_max-depth_prediction_min,\n",
    "    # ))\n",
    "\n",
    "    #####################################\n",
    "    # making output statistics of objects\n",
    "    output[image][\"objects\"] = {}\n",
    "    output[image][\"stats\"] = {}\n",
    "    output[image][\"stats\"][\"depth\"] = {}\n",
    "    output[image][\"stats\"][\"depth\"][\"min\"] = depth_prediction_min\n",
    "    output[image][\"stats\"][\"depth\"][\"max\"] = depth_prediction_max\n",
    "    for obj in range(curr_objects_num):\n",
    "      obj_x1, obj_y1, obj_x2, obj_y2 = curr_pred_boxes[obj].cpu().numpy().astype(\"int\")\n",
    "      obj_xc, obj_yc = math.ceil((obj_x1+obj_x2)/2), math.ceil((obj_y1+obj_y2)/2)\n",
    "      dot_position = (obj_xc, obj_yc)\n",
    "      depth_prediction_curr_obj = depth_predictions[\"depth\"].getpixel(dot_position)\n",
    "      curr_igg  = cv2.rectangle(curr_igg, (obj_x1, obj_y1), (obj_x2, obj_y2), boxcolor, 1)\n",
    "      class_name = coco_names[curr_pred_labels.cpu().numpy()[obj]-1]\n",
    "      curr_objects.append(class_name)\n",
    "      curr_igg = cv2.putText(curr_igg, class_name, (obj_x1, obj_y1-6), font, 0.5, textcolor, 1, antialiasedline)\n",
    "      curr_igg = cv2.putText(curr_igg, \"x\", (obj_xc, obj_yc), font, 0.2, centercolor, 1, antialiasedline)\n",
    "\n",
    "      ################################\n",
    "      # find actual size of the object\n",
    "      obj_area = rectAreaFromCoordinates(obj_x1, obj_y1, obj_x2, obj_y2)\n",
    "      obj_area_org = obj_area\n",
    "      for step in range(depth_prediction_curr_obj, depth_prediction_min, -1):\n",
    "        obj_area_org = obj_multiplicative_step*obj_area_org\n",
    "      obj_area_org = math.ceil(obj_area_org)\n",
    "\n",
    "      #####################\n",
    "      # storing output json\n",
    "      if class_name not in output[image][\"objects\"]:\n",
    "        output[image][\"objects\"][class_name] = {}\n",
    "      curr_obj_index = len(output[image][\"objects\"][class_name].keys())\n",
    "      output[image][\"objects\"][class_name][curr_obj_index] = {}\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"x1\"]    = obj_x1\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"y1\"]    = obj_y1\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"x2\"]    = obj_x2\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"y2\"]    = obj_y2\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"xc\"]    = obj_xc\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"yc\"]    = obj_yc\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"class\"] = coco_names.index(class_name)\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"area\"]  = obj_area_org\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"depth\"] = depth_prediction_curr_obj\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"score\"] = round(curr_pred_scores[obj].item(), 2)\n",
    "\n",
    "      output[image][\"objects\"][class_name][curr_obj_index][\"primary\"] = 1 # random.getrandbits(1)\n",
    "      if (depth_prediction_curr_obj > obj_ignore_depth):\n",
    "        output[image][\"objects\"][class_name][curr_obj_index][\"primary\"] = 0\n",
    "        if (obj_area < obj_ignore_area):\n",
    "          output[image][\"objects\"][class_name][curr_obj_index][\"primary\"] = 0\n",
    "        else:\n",
    "          pass\n",
    "      else:\n",
    "        if (obj_area < obj_ignore_area):\n",
    "          output[image][\"objects\"][class_name][curr_obj_index][\"primary\"] = 0\n",
    "        else:\n",
    "          pass\n",
    "      draw.point(dot_position, fill = \"white\")\n",
    "    curr_objects = list(dict.fromkeys(curr_objects))\n",
    "\n",
    "    # ####################################\n",
    "    # # making output statistics of scenes\n",
    "    # output[image][\"scenes\"] = {}\n",
    "    # for scene in scenes:\n",
    "    #   for obj in curr_objects:\n",
    "    #     if (obj in scenes[scene]):\n",
    "    #       if scene not in output[image][\"scenes\"]:\n",
    "    #         output[image][\"scenes\"][scene] = {}\n",
    "    #       output[image][\"scenes\"][scene][obj] = 0\n",
    "    #       for obj_i in output[image][\"objects\"][obj]:\n",
    "    #         output[image][\"scenes\"][scene][obj] += 1\n",
    "\n",
    "  ##########################################\n",
    "  # copying original-image to dataset_folder\n",
    "  target_img_dest = \"{:}/{:}/{:}-{:}.jpg\".format(dataset_folder, img_folder_originals, image, timestamp)\n",
    "  shutil.copyfile(target_img, target_img_dest)\n",
    "\n",
    "  ###############################\n",
    "  # saving object-detection-image\n",
    "  curr_igg_image = Image.fromarray(curr_igg)\n",
    "  target_img_object_detection = \"{:}/{:}/{:}-{:}.jpg\".format(main_folder, image, timestamp, object_detection_file_postfix)\n",
    "  curr_igg_image.save(target_img_object_detection)\n",
    "  # copying object-detection-image to dataset_folder\n",
    "  target_img_object_detection_dest = \"{:}/{:}/{:}-{:}.jpg\".format(dataset_folder, img_folder_objects, image, timestamp)\n",
    "  shutil.copyfile(target_img_object_detection, target_img_object_detection_dest)\n",
    "\n",
    "  ####################\n",
    "  # saving depth-image\n",
    "  target_img_depth = \"{:}/{:}/{:}-{:}.jpg\".format(main_folder, image, timestamp, depth_file_postfix)\n",
    "  depth_predictions[\"depth\"].save(target_img_depth)\n",
    "  # copying depth-image to dataset_folder\n",
    "  target_img_depth_dest = \"{:}/{:}/{:}-{:}.jpg\".format(dataset_folder, img_folder_depths, image, 0)\n",
    "  shutil.copyfile(target_img_depth, target_img_depth_dest)\n",
    "\n",
    "  ######################################\n",
    "  # saving object details in a text file\n",
    "  txt_file_cols = {\n",
    "      \"x1\"         : \"x1\",\n",
    "      \"y1\"         : \"x2\",\n",
    "      \"x2\"         : \"y1\",\n",
    "      \"y2\"         : \"y2\",\n",
    "      \"xc\"         : \"xc\",\n",
    "      \"yc\"         : \"yc\",\n",
    "      \"class\"      : \"class\",\n",
    "      \"area\"       : \"area\",\n",
    "      \"depth\"      : \"depth\",\n",
    "      \"score\"      : \"pred_score\",\n",
    "      \"primary\"    : \"primary\",\n",
    "  }\n",
    "  target_txt_object = \"{:}/{:}/{:}.txt\".format(main_folder, image, 0)\n",
    "  with open(target_txt_object, \"w\") as txt_file:\n",
    "    txt_file.write(\n",
    "      \",\".join([txt_file_cols[key] for key in txt_file_cols.keys()])+\"\\n\"\n",
    "    )\n",
    "    for ind, obj in enumerate(output[image][\"objects\"]):\n",
    "      occ_obj = len(output[image][\"objects\"][obj])\n",
    "      print (\" - {:}. {:} ({:}):\".format(\n",
    "        ind,\n",
    "        obj,\n",
    "        occ_obj,\n",
    "      ))\n",
    "      for obj_i in output[image][\"objects\"][obj]:\n",
    "        txt_file.write(\n",
    "          \",\".join([str(output[image][\"objects\"][obj][obj_i][key]) for key in txt_file_cols.keys()])+\"\\n\"\n",
    "        )\n",
    "        print (\"    - #{:} - area:{:}, depth:{:}, score:{:.3f}, (x1, y1): {:}, (x2, y2): {:}, (xc, yc): {:} - PRIMARY: {:}\".format(\n",
    "            obj_i,\n",
    "            output[image][\"objects\"][obj][obj_i][\"area\"],\n",
    "            output[image][\"objects\"][obj][obj_i][\"depth\"],\n",
    "            output[image][\"objects\"][obj][obj_i][\"score\"],\n",
    "            \"({:}, {:})\".format(output[image][\"objects\"][obj][obj_i][\"x1\"], output[image][\"objects\"][obj][obj_i][\"y1\"]),\n",
    "            \"({:}, {:})\".format(output[image][\"objects\"][obj][obj_i][\"x2\"], output[image][\"objects\"][obj][obj_i][\"y2\"]),\n",
    "            \"({:}, {:})\".format(output[image][\"objects\"][obj][obj_i][\"xc\"], output[image][\"objects\"][obj][obj_i][\"yc\"]),\n",
    "            output[image][\"objects\"][obj][obj_i][\"primary\"],\n",
    "        ))\n",
    "  target_txt_object_dest = \"{:}/{:}/{:}-{:}.txt\".format(dataset_folder, txt_folder_objects, image, 0)\n",
    "  shutil.copyfile(target_txt_object, target_txt_object_dest)\n",
    "\n",
    "  # ###################################################\n",
    "  # # displaying object-detection-image and depth-image\n",
    "  # if (showrgb == True):\n",
    "  #   cv2_imshow(rgb2gray(curr_igg))\n",
    "  # else:\n",
    "  #   cv2_imshow(curr_igg)\n",
    "  # depth_predictions[\"depth\"].show()\n",
    "\n",
    "  clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_csv_file = \"{:}/custom_annotations.txt\".format(dataset_folder)\n",
    "with open(target_csv_file, \"w\") as csv_file:\n",
    "    csv_file.write(\"{:},{:},{:},{:},{:}\\n\".format(\n",
    "        \"image_id\",\n",
    "        \"image_original\",\n",
    "        \"image_object_detection\",\n",
    "        \"image_depth\",\n",
    "        \"text_object\",\n",
    "    ))\n",
    "    for image in range(images):\n",
    "        timestamp = 0\n",
    "        image_original = \"{:}/{:}/{:}-{:}.jpg\".format(\n",
    "            dataset_folder, img_folder_originals, image, timestamp\n",
    "        )\n",
    "        image_object_detection = \"{:}/{:}/{:}-{:}.jpg\".format(\n",
    "            dataset_folder, img_folder_objects, image, timestamp\n",
    "        )\n",
    "        image_depth = \"{:}/{:}/{:}-{:}.jpg\".format(\n",
    "            dataset_folder, img_folder_depths, image, timestamp\n",
    "        )\n",
    "        text_object = \"{:}/{:}/{:}-{:}.txt\".format(\n",
    "            dataset_folder, txt_folder_objects, image, timestamp\n",
    "        )\n",
    "        csv_file.write(\"{:},{:},{:},{:},{:}\\n\".format(\n",
    "            image,\n",
    "            image_original,\n",
    "            image_object_detection,\n",
    "            image_depth,\n",
    "            text_object,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqGl42MVz5aU"
   },
   "source": [
    "## Zip 'dataset' and 'images' folders and download the zip(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "4OrWFC3M3EzB"
   },
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now(pytz.timezone('Asia/Seoul'))\n",
    "timestamp_string = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "B-xSVZa2hCZK",
    "outputId": "25e31332-84c3-4d7e-e7d9-0f5c67d39a0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/malik/Artificial-Retina/code/images_2023-07-05_20-16-05.zip'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the dataset folder\n",
    "shutil.make_archive(dataset_folder+\"_\"+timestamp_string, 'zip', dataset_folder)\n",
    "\n",
    "# zip the images folder\n",
    "shutil.make_archive(main_folder+\"_\"+timestamp_string, 'zip', main_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzvx8YL7K3rJ"
   },
   "source": [
    "### depth to depth_prediction_max (mostly 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJ_KYFHpD9Xx"
   },
   "outputs": [],
   "source": [
    "# curr_obj_area_org = output[image][\"objects\"][obj][obj_i][\"area\"]\n",
    "# for step in range(output[image][\"objects\"][obj][obj_i][\"depth\"], depth_prediction_max, 1):\n",
    "#   curr_obj_area_org = obj_multiplicative_step*curr_obj_area_org\n",
    "#   print (\"{:} ({:})\".format(\n",
    "#       step, curr_obj_area_org\n",
    "#   ),\n",
    "#   end=\", \")\n",
    "# print (curr_obj_area_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zuQ7SPnLExQ"
   },
   "source": [
    "### depth to depth_prediction_min (mostly close to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrKx_FdxLE8m"
   },
   "outputs": [],
   "source": [
    "# curr_obj_area_org = output[image][\"objects\"][obj][obj_i][\"area\"]\n",
    "# for step in range(output[image][\"objects\"][obj][obj_i][\"depth\"], depth_prediction_min, -1):\n",
    "#   curr_obj_area_org = obj_multiplicative_step*curr_obj_area_org\n",
    "#   print (\"{:} ({:})\".format(\n",
    "#       step, curr_obj_area_org\n",
    "#   ),\n",
    "#   end=\", \")\n",
    "# print (curr_obj_area_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oloYmaiNwAm"
   },
   "source": [
    "### find min and max depth values of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwbzOAV_89SU"
   },
   "outputs": [],
   "source": [
    "# depth_prediction_min = float('inf')\n",
    "# depth_prediction_max = float('-inf')\n",
    "# depth_prediction_w, depth_prediction_h = depth_predictions[\"depth\"].size\n",
    "# print (\" - The image has '{:}' rows and '{:}' columns.\".format(depth_prediction_w, depth_prediction_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6mXujZA7hra"
   },
   "outputs": [],
   "source": [
    "# # Iterate over each pixel in the image to find out minimum and maximum value\n",
    "# for y in range(depth_prediction_h):\n",
    "#   for x in range(depth_prediction_w):\n",
    "#     depth_prediction_pv = depth_predictions[\"depth\"].getpixel((x, y))\n",
    "#     if depth_prediction_pv < depth_prediction_min:\n",
    "#       depth_prediction_min = depth_prediction_pv\n",
    "#     if depth_prediction_pv > depth_prediction_max:\n",
    "#       depth_prediction_max = depth_prediction_pv\n",
    "\n",
    "# depth_prediction_minmax = depth_prediction_min, depth_prediction_max\n",
    "# print (\" - The min and max depths of this image is {:}. Diff: {:}\".format(\n",
    "#     depth_prediction_minmax,\n",
    "#     depth_prediction_max-depth_prediction_min,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBvGIPLiDNsE"
   },
   "source": [
    "## **Displaying output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "sGI9SE_pmlAP",
    "outputId": "866d2450-ca90-447a-b855-b2c3ac51347c"
   },
   "outputs": [],
   "source": [
    "# # iterating and displaying full output (scenes, objects)\n",
    "# # scenes_in_all_images = []\n",
    "# for image in output:\n",
    "#   # print (\"\")\n",
    "#   print (\"Image # {:}:\".format(image))\n",
    "\n",
    "#   indent_for_object_scenes = \"   \"\n",
    "#   indent_for_object_stats  = \"\\t\"\n",
    "\n",
    "#   # ###################\n",
    "#   # # displaying scenes\n",
    "#   # print (\"\\n- Scenes:\")\n",
    "#   # for ind, scene in enumerate(output[image][\"scenes\"]):\n",
    "#   #   print (\"{:}{:}. {:}:\\t\\t({:})\".format(\n",
    "#   #     indent_for_object_scenes,\n",
    "#   #     ind,\n",
    "#   #     scene,\n",
    "#   #     # output[image][\"scenes\"][scene],\n",
    "#   #     ', '.join('{}:{}'.format(key, value) for key, value in output[image][\"scenes\"][scene].items())\n",
    "#   #   ))\n",
    "\n",
    "#   ####################\n",
    "#   # displaying objects\n",
    "#   num_objs = len(output[image][\"objects\"])\n",
    "#   print (\"- Objects ({:}):\".format(num_objs))\n",
    "#   for ind, obj in enumerate(output[image][\"objects\"]):\n",
    "#     occ_obj = len(output[image][\"objects\"][obj])\n",
    "#     print (\"{:}{:}. {:} ({:}):\".format(\n",
    "#       indent_for_object_scenes,\n",
    "#       ind,\n",
    "#       obj.capitalize(),\n",
    "#       occ_obj,\n",
    "#     ))\n",
    "#     for obj_i in output[image][\"objects\"][obj]:\n",
    "#       print (\"{:} #{:} - area:{:}, depth:{:}, score:{:.3f}, class:{:}, (x1, y1): {:}, (x2, y2): {:}, (xc, yc): {:}\".format(\n",
    "#           indent_for_object_stats, obj_i,\n",
    "#           output[image][\"objects\"][obj][obj_i][\"area\"],\n",
    "#           output[image][\"objects\"][obj][obj_i][\"depth\"],\n",
    "#           output[image][\"objects\"][obj][obj_i][\"score\"],\n",
    "#           output[image][\"objects\"][obj][obj_i][\"class\"],\n",
    "#           \"({:}, {:})\".format(output[image][\"objects\"][obj][obj_i][\"x1\"], output[image][\"objects\"][obj][obj_i][\"y1\"]),\n",
    "#           \"({:}, {:})\".format(output[image][\"objects\"][obj][obj_i][\"x2\"], output[image][\"objects\"][obj][obj_i][\"y2\"]),\n",
    "#           \"({:}, {:})\".format(output[image][\"objects\"][obj][obj_i][\"xc\"], output[image][\"objects\"][obj][obj_i][\"yc\"]),\n",
    "#       ))\n",
    "\n",
    "#   # scenes_in_curr_image = len(output[image][\"scenes\"])\n",
    "#   # scenes_in_all_images.append(scenes_in_curr_image)\n",
    "\n",
    "#   print (\"\\n******************\\n\")\n",
    "\n",
    "# # ###########################################\n",
    "# # # displaying number of scenes in each image\n",
    "# # for image, num_of_scenes in enumerate(scenes_in_all_images):\n",
    "# #   print (\"Image # {:} has '{:}' scenes.\".format(image, num_of_scenes), end = \"\")\n",
    "# #   if (num_of_scenes == 0 or num_of_scenes > 1):\n",
    "# #     print (\" (have a look at this image number)\")\n",
    "\n",
    "# #   else:\n",
    "# #     print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxiOcLdU5288"
   },
   "outputs": [],
   "source": [
    "# output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuqhAeIE6iOF"
   },
   "outputs": [],
   "source": [
    "# ###########################################\n",
    "# # displaying number of scenes in each image\n",
    "# for image, num_of_scenes in enumerate(scenes_in_all_images):\n",
    "#   print (\"\\nImage # {:} has '{:}' scene(s).\".format(image, num_of_scenes), end = \"\")\n",
    "#   if (num_of_scenes == 0 or num_of_scenes > 1):\n",
    "#     print (\" (have a look at this image number)\")\n",
    "#     for ind, scene in enumerate(output[image][\"scenes\"]):\n",
    "#       print (\"{:}{:}. {:}({:} objects):\\t\\t({:})\".format(\n",
    "#         indent_for_object_scenes,\n",
    "#         ind,\n",
    "#         scene,\n",
    "#         len(output[image][\"scenes\"][scene]),\n",
    "#         # output[image][\"scenes\"][scene],\n",
    "#         \", \".join(\"{:}:{:}\".format(key, value) for key, value in output[image][\"scenes\"][scene].items())\n",
    "#       ))\n",
    "#       # for obj_in_scene in output[image][\"scenes\"][scene]:\n",
    "#       #   print (\"  - {:}{:}\".format(indent_for_object_scenes, obj_in_scene))\n",
    "#       # print (\"\")\n",
    "#   else:\n",
    "#     print (\" (ok)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsN91uMnLim7"
   },
   "source": [
    "## **Viewing one sample output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpHZqpyfwXLK"
   },
   "outputs": [],
   "source": [
    "# output_i = 0\n",
    "# if (output_i >= images_to_consider):\n",
    "#   output_i = 0\n",
    "# print (\"Sample output for Image # {:}\".format(output_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i86YokNepCCd"
   },
   "outputs": [],
   "source": [
    "# output[output_i][\"scenes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZ70VJdhUvic"
   },
   "outputs": [],
   "source": [
    "# output[output_i][\"objects\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk_UUd4tyWG5"
   },
   "outputs": [],
   "source": [
    "# # print (\"\\n- Scenes:\")\n",
    "# # for ind, scene in enumerate(output[output_i][\"scenes\"]):\n",
    "# #   print (\"{:}{:}. {:}:\\t\\t({:})\".format(\n",
    "# #     indent_for_object_scenes,\n",
    "# #     ind,\n",
    "# #     scene,\n",
    "# #     # output[image][\"scenes\"][scene],\n",
    "# #     ', '.join('{}:{}'.format(key, value) for key, value in output[output_i][\"scenes\"][scene].items())\n",
    "# #   ))\n",
    "\n",
    "# target_image_sample       = \"{:}/{:}/{:}-{:}.jpg\".format(main_folder, output_i, 0, object_detection_file_postfix)\n",
    "# target_image_depth_sample = \"{:}/{:}/{:}-{:}.jpg\".format(main_folder, output_i, 0, depth_file_postfix)\n",
    "# target_image_sample_output       = Image.open(target_image_sample)\n",
    "# target_image_depth_sample_output = Image.open(target_image_depth_sample)\n",
    "# target_image_sample_output.show()\n",
    "# target_image_depth_sample_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M2Gq3H_VD5x"
   },
   "source": [
    "## **PyTorch model structures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shfzNgbdz4_d"
   },
   "source": [
    "### **depth_estimator** model 1 (a) structure\n",
    "model:\n",
    "* **glpn**\n",
    "  * encoder\n",
    "* **decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJPK5A5I0AQ3"
   },
   "outputs": [],
   "source": [
    "depth_estimator.model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r7X7Zt3tucv"
   },
   "source": [
    "### **depth_estimator2** model 1 (b) structure\n",
    "model:\n",
    "* **conv2d**\n",
    "  * conv2d-one\n",
    "* **two**\n",
    "  * two-one\n",
    "* **three**\n",
    "  * three-one\n",
    "* **four**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjDHhL-8zVIG"
   },
   "outputs": [],
   "source": [
    "depth_estimator2.conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQi3gRrEx6We"
   },
   "source": [
    "### **object_detector** model 2 structure\n",
    "model\n",
    "* **backbone**\n",
    "  * features\n",
    "  * extra\n",
    "* **anchor_generator**\n",
    "  * aspect_ratios\n",
    "* **head**\n",
    "  * classification_head\n",
    "* **transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9Ml2uoM3hgh"
   },
   "outputs": [],
   "source": [
    "object_detector.backbone.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDtn6QMyVkQs"
   },
   "outputs": [],
   "source": [
    "tmp_layers = [type(layer) for layer in object_detector.backbone.features]\n",
    "tmp_layers_unq = set(tmp_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuOqGV2XXn_l"
   },
   "outputs": [],
   "source": [
    "print (\"Types of layers used in 'object_detector' model:\")\n",
    "for tmp in tmp_layers_unq:\n",
    "  print (\"  -\", tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0N8LEmkVXrI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Types (continuous, ordinal, catgeorical)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M2PsgcLOaOy9"
   },
   "outputs": [],
   "source": [
    "dataset_defintion = {\n",
    "    \"image\": {\n",
    "        \"originals\": {\"values\": {\"min\": 0, \"max\": 255}, \"category\": \"image-continous\"},\n",
    "        \"objects\":   {\"values\": {\"min\": 0, \"max\": 255}, \"category\": \"image-continous\"},\n",
    "        \"depths\":    {\"values\": {\"min\": 0, \"max\": 255}, \"category\": \"image-continous\"},\n",
    "    },\n",
    "    \"objectspecs\": {\n",
    "        \"x1\": { \"values\": {\"min\": 0, \"max\": 255}, \"d-type\": \"int\", \"category\": \"continous/numerical\"},\n",
    "        \"x2\": { \"values\": {\"min\": 0, \"max\": 255}, \"d-type\": \"int\", \"category\": \"continous/numerical\"},\n",
    "        \"y1\": { \"values\": {\"min\": 0, \"max\": 255}, \"d-type\": \"int\", \"category\": \"continous/numerical\"},\n",
    "        \"y2\": { \"values\": {\"min\": 0, \"max\": 255}, \"d-type\": \"int\", \"category\": \"continous/numerical\"},\n",
    "        \"xc\": { \"values\": {\"min\": 0, \"max\": 255}, \"d-type\": \"int\", \"category\": \"continous/numerical\"},\n",
    "        \"yc\": { \"values\": {\"min\": 0, \"max\": 255}, \"d-type\": \"int\", \"category\": \"continous/numerical\"},\n",
    "        \"class\": { \"values\": {\"min\": 0, \"max\": 90}, \"d-type\": \"int\", \"category\": \"categorical/nominal\"},\n",
    "        \"area\": { \"values\": {\"min\": 0, \"max\": 65536}, \"d-type\": \"float\", \"category\": \"continous/numerical\"},\n",
    "        \"depth\": { \"values\": {\"min\": 0, \"max\": 255}, \"d-type\": \"int\", \"category\": \"continous/numerical\"},\n",
    "        \"pred_score\": { \"values\": {\"min\": 0, \"max\": 1}, \"d-type\": \"float\", \"category\": \"continous/numerical\"},\n",
    "        \"primary\": { \"values\": {\"min\": 0, \"max\": 1}, \"d-type\": \"bool\", \"category\": \"categorical/nominal\"},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_analysis_techniques = {\n",
    "    \"continous/numerical\": {\n",
    "        \"DescriptivE-StatisticS to provide a summary of main features of the data\": [\n",
    "            \"main\", \"median\", \"standard-deviation\", \"range\", \"percentiles\", \"central-tendency\", \"dispersion\",\n",
    "        ],\n",
    "        \"DatA-VisualizatioN to explore distribution, patterns, and relationships\": [\n",
    "            \"histograms\", \"box-plots\", \"scatter-plots\", \"line-plots\",\n",
    "        ],\n",
    "        \"InferencE-StatisticS to make inferences about population based on sample data\": [\n",
    "            \"hypothesis-testing(t-tests, ANOVA)\",\n",
    "        ],\n",
    "        \"CorrelatioN-AnalysiS to assess the strength and direction of relationship between continous variables\" : [\n",
    "            \"correlation-coefficients\", \"pearson-correlation\",\n",
    "        ],\n",
    "        \"RegressioN-AnalysiS to model the relationship between a dependent variable and one or more independent variables\": [\n",
    "            \"linear\", \"polynomial\", \"logistic\", \"poisson\", \"ridge\", \"lasso\", \"elastic-net\",\n",
    "            \"stepwise\", \"robust\", \"non-linear\", \"time-series\", \"hierarchical\", \"bayesian\",\n",
    "        ],\n",
    "        \"TimE-SerieS-AnalysiS for the data collected over time\": [\n",
    "            \"trends\", \"seasonality\", \"patterns\",\n",
    "        ],\n",
    "        \"ProbabilitY/NormaL-DistributionS to model underlying distribution of continuous data\": [\n",
    "            \"likelihood of observing certain values\",\n",
    "        ],\n",
    "        \"Outlier Detection to ensure robustness of the analysis\": [\n",
    "            \"indicates-data-quality-issues-or-interesting-phenomena\",\n",
    "        ],\n",
    "        \"Clustering Analysis to group data points with similar characteristics\": [\n",
    "            \"distinct-patterns\", \"segments-within-data\",\n",
    "        ],\n",
    "        \"DimensionalitY-ReductioN to reduce dimensions of data and preserve important patterns\": [\n",
    "            \"prinicpal-component-analysis(pca)\", \"t-distributed-stochastic-neighbor-embedding(t-SNE)\",\n",
    "        ],\n",
    "    },\n",
    "    \"ordinal/categorical\": {\n",
    "        \"DescriptivE-StatisticS to provide a summary of category distribution\": [\n",
    "            \"frequency\", \"percentages\", \"central-tendency(median/mode)\",\n",
    "        ],\n",
    "        \"NoN-ParametriC-TestS to provide a summary of category distribution\": [\n",
    "            \"Mann-Whitney-U\", \"Kruskal-Wallis\", \"Wilcoxon-signed-rank\",\n",
    "        ],\n",
    "        \"OrdinaL-LogistiC-RegressioN to model relationship b/w ordinal-outcomes and other ordianl/nominal outcomes\": [\n",
    "            \"proportional-odds-model\", \"ordinal-outcome-level-changing\",\n",
    "        ],\n",
    "        \"Spearman'S-RanK-CorrelatioN to assess the strength and direction of relationship b/w two ordinal variables\": [\n",
    "            \"non-parametric-alternative-to-Pearson-correlation\", \"suitable-for-monotonic-relationships\",\n",
    "        ],\n",
    "        \"ProportionaL-OddS-ModeL to analyze ordinal data with multiple predictor variables\": [\n",
    "            \"estimates the odds of an ordinal outcome being in a higher category, given the predictor variables\",\n",
    "        ],\n",
    "        \"CrosS-TabulatioN-AnD-Chi-SquarE-TesT to identify associations b/w 2/more categorical variables, including ordinals\": [\n",
    "            \"\",\n",
    "        ],\n",
    "        \"VisualizationS to display the distribution of ordinal data and revealing patterns between variables\": [\n",
    "            \"bar-charts\", \"stacked-bar-charts\", \"mosaic-plots\",\n",
    "        ],\n",
    "        \"CumulativE-ProbabilitY-PlotS to visualize the cumulative distribution of ordinal data\": [\n",
    "            \"insights-into-the-underlying-structure\", \"comparisons-between-groups\",\n",
    "        ],\n",
    "    },\n",
    "    \"categorical/nominal\": {\n",
    "        \"Descriptive Statistics\": [\"TBD\"],\n",
    "        \"NoN-ParametriC-TestS\": [\n",
    "            \"Mann-Whitney-U\", \"Kruskal-Wallis\", \"Wilcoxon-signed-rank\",\n",
    "        ],\n",
    "        \"Ordinal Logistic Regression\": [\"TBD\"],\n",
    "        \"Spearman's Rank Correlation\": [\"TBD\"],\n",
    "        \"Proportional Odds Model\": [\"TBD\"],\n",
    "        \"Cross-Tabulation and Chi-Square Test\": [\"TBD\"],\n",
    "        \"Visualizations\": [\n",
    "            \"bar-charts\", \"stacked-bar-charts\", \"mosaic-plots\",\n",
    "        ],\n",
    "        \"Cumulative Probability Plots\": [\"TBD\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply these statistical analysis if one of these:\n",
      "\n",
      "1. continous/numerical:\n",
      "  - DescriptivE-StatisticS to provide a summary of main features of the data:\n",
      "    - main, median, standard-deviation, range, percentiles, central-tendency and dispersion\n",
      "  - DatA-VisualizatioN to explore distribution, patterns, and relationships:\n",
      "    - histograms, box-plots, scatter-plots and line-plots\n",
      "  - InferencE-StatisticS to make inferences about population based on sample data:\n",
      "    - hypothesis-testing(t-tests and ANOVA)\n",
      "  - CorrelatioN-AnalysiS to assess the strength and direction of relationship between continous variables:\n",
      "    - correlation-coefficients and pearson-correlation\n",
      "  - RegressioN-AnalysiS to model the relationship between a dependent variable and one or more independent variables:\n",
      "    - linear, polynomial, logistic, poisson, ridge, lasso, elastic-net, stepwise, robust, non-linear, time-series, hierarchical and bayesian\n",
      "  - TimE-SerieS-AnalysiS for the data collected over time:\n",
      "    - trends, seasonality and patterns\n",
      "  - ProbabilitY/NormaL-DistributionS to model underlying distribution of continuous data:\n",
      "    - likelihood of observing certain values\n",
      "  - Outlier Detection to ensure robustness of the analysis:\n",
      "    - indicates-data-quality-issues-or-interesting-phenomena\n",
      "  - Clustering Analysis to group data points with similar characteristics:\n",
      "    - distinct-patterns and segments-within-data\n",
      "  - DimensionalitY-ReductioN to reduce dimensions of data and preserve important patterns:\n",
      "    - prinicpal-component-analysis(pca) and t-distributed-stochastic-neighbor-embedding(t-SNE)\n",
      "\n",
      "2. ordinal/categorical:\n",
      "  - DescriptivE-StatisticS to provide a summary of category distribution:\n",
      "    - frequency, percentages and central-tendency(median/mode)\n",
      "  - NoN-ParametriC-TestS to provide a summary of category distribution:\n",
      "    - Mann-Whitney-U, Kruskal-Wallis and Wilcoxon-signed-rank\n",
      "  - OrdinaL-LogistiC-RegressioN to model relationship b/w ordinal-outcomes and other ordianl/nominal outcomes:\n",
      "    - proportional-odds-model and ordinal-outcome-level-changing\n",
      "  - Spearman'S-RanK-CorrelatioN to assess the strength and direction of relationship b/w two ordinal variables:\n",
      "    - non-parametric-alternative-to-Pearson-correlation and suitable-for-monotonic-relationships\n",
      "  - ProportionaL-OddS-ModeL to analyze ordinal data with multiple predictor variables:\n",
      "    - estimates the odds of an ordinal outcome being in a higher category and given the predictor variables\n",
      "  - CrosS-TabulatioN-AnD-Chi-SquarE-TesT to identify associations b/w 2/more categorical variables, including ordinals:\n",
      "    - \n",
      "  - VisualizationS to display the distribution of ordinal data and revealing patterns between variables:\n",
      "    - bar-charts, stacked-bar-charts and mosaic-plots\n",
      "  - CumulativE-ProbabilitY-PlotS to visualize the cumulative distribution of ordinal data:\n",
      "    - insights-into-the-underlying-structure and comparisons-between-groups\n",
      "\n",
      "3. categorical/nominal:\n",
      "  - Descriptive Statistics:\n",
      "    - TBD\n",
      "  - NoN-ParametriC-TestS:\n",
      "    - Mann-Whitney-U, Kruskal-Wallis and Wilcoxon-signed-rank\n",
      "  - Ordinal Logistic Regression:\n",
      "    - TBD\n",
      "  - Spearman's Rank Correlation:\n",
      "    - TBD\n",
      "  - Proportional Odds Model:\n",
      "    - TBD\n",
      "  - Cross-Tabulation and Chi-Square Test:\n",
      "    - TBD\n",
      "  - Visualizations:\n",
      "    - bar-charts, stacked-bar-charts and mosaic-plots\n",
      "  - Cumulative Probability Plots:\n",
      "    - TBD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Apply these statistical analysis if one of these:\\n\")\n",
    "indentation1 = \"  \"\n",
    "indentation2 = indentation1+indentation1\n",
    "for i, analysis_tech in enumerate(dataset_analysis_techniques, 1):\n",
    "    print (\"{:}. {:}:\".format(i, analysis_tech))\n",
    "    for analysis_meth in dataset_analysis_techniques[analysis_tech]:\n",
    "        print (\"{:}- {:}:\\n{:}- {:}\".format(\n",
    "            indentation1, analysis_meth,\n",
    "            indentation2,\n",
    "            \", \".join(dataset_analysis_techniques[analysis_tech][analysis_meth])[::-1].replace(\",\", \"dna \", 1)[::-1],\n",
    "        ))\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "m4RNuk3rzsll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image\n",
      "- originals (image-continous)\tanalysis-method: not-sure\n",
      "- objects (image-continous)\tanalysis-method: not-sure\n",
      "- depths (image-continous)\tanalysis-method: not-sure\n",
      "\n",
      "objectspecs\n",
      "- x1 (continous)\tanalysis-method: statistical techniques\n",
      "- x2 (continous)\tanalysis-method: statistical techniques\n",
      "- y1 (continous)\tanalysis-method: statistical techniques\n",
      "- y2 (continous)\tanalysis-method: statistical techniques\n",
      "- xc (continous)\tanalysis-method: statistical techniques\n",
      "- yc (continous)\tanalysis-method: statistical techniques\n",
      "- class (category)\tanalysis-method: not-sure\n",
      "- area (continous)\tanalysis-method: statistical techniques\n",
      "- depth (continous)\tanalysis-method: statistical techniques\n",
      "- pred_score (continous)\tanalysis-method: statistical techniques\n",
      "- primary (category)\tanalysis-method: not-sure\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for definition in dataset_defintion:\n",
    "    print (definition)\n",
    "    for subdef in dataset_defintion[definition]:\n",
    "        subdefcat = dataset_defintion[definition][subdef][\"category\"]\n",
    "        subdefmethod = \"not-sure\"\n",
    "        if (subdefcat == \"continous\"):\n",
    "            subdefmethod = \"statistical techniques\"\n",
    "        elif (subdefcat == \"ordinal\"):\n",
    "            subdefmethod = \"non-parametric statistical methods\"\n",
    "        elif (subdefcat == \"categorical\"):\n",
    "            subdefmethod = \"descriptive statistics (frequency-counts, bar-charts) and statistical tests (chi-square)\"\n",
    "        print (\"- {:} ({:})\".format(subdef, subdefcat), end=\"\")\n",
    "        print (\"\\tanalysis-method: {:}\".format(subdefmethod))\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTocftzQYd8s"
   },
   "source": [
    "## Generating part by JUNGBEOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyKgjVJYzsol"
   },
   "outputs": [],
   "source": [
    "!mkdir dataset\n",
    "!mkdir dataset/images\n",
    "!mkdir dataset/dpe_images\n",
    "!mkdir dataset/objects\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_file = \"dataset/annotations.csv\"\n",
    "original_folder = 'dataset/images'\n",
    "depth_folder = 'dataset/dpe_images'\n",
    "txt_folder = 'dataset/objects'\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    \"Previous annotation file has benn removed.\"\n",
    "    os.remove(csv_file)\n",
    "\n",
    "transform = T.ToTensor()\n",
    "output = {}\n",
    "\n",
    "\n",
    "# The score threshold needs to be decided\n",
    "score_threshold = 0\n",
    "\n",
    "for image in range(images):\n",
    "    output[image] = {}\n",
    "    target_img = \"{}/{:}/{:}.jpg\".format(main_folder, image, 0)\n",
    "    print(f'Processing {target_img}')\n",
    "    curr_ig = Image.open(target_img)\n",
    "    # flow 1 complete\n",
    "    curr_img = transform(curr_ig)\n",
    "\n",
    "    # flow 2 complete\n",
    "    with torch.no_grad():\n",
    "        # print (curr_img.shape)\n",
    "        curr_pred = object_detector([curr_img])\n",
    "        curr_pred_keys = np.array(list(curr_pred[0].keys()))\n",
    "        curr_pred_num_keys = len(curr_pred_keys)\n",
    "        curr_pred_boxes, curr_pred_scores, curr_pred_labels = curr_pred[0][\"boxes\"], curr_pred[0][\"scores\"], curr_pred[0][\"labels\"]\n",
    "\n",
    "\n",
    "        curr_objects_num = torch.sum(curr_pred_scores >= obj_det_probability_threshold)\n",
    "        curr_objects = []\n",
    "        curr_igg = cv2.imread(target_img)\n",
    "\n",
    "        output[image][\"objects\"] = {}\n",
    "        for obj in range(curr_objects_num):\n",
    "            obj_x1, obj_y1, obj_x2, obj_y2 = curr_pred_boxes[obj].numpy().astype(\"int\")\n",
    "            curr_igg  = cv2.rectangle(curr_igg, (obj_x1, obj_y1), (obj_x2, obj_y2), boxcolor, 1)\n",
    "            class_name = coco_names[curr_pred_labels.numpy()[obj]-1]\n",
    "            curr_objects.append(class_name)\n",
    "            curr_igg = cv2.putText(curr_igg, class_name, (obj_x1, obj_y1-6), font, 0.5, textcolor, 1, antialiasedline)\n",
    "            obj_area = rectAreaFromCoordinates(obj_x1, obj_y1, obj_x2, obj_y2)\n",
    "            # print (\"'{:}' area: {:}\".format(class_name, obj_area))\n",
    "            #############################################################\n",
    "            if class_name not in output[image][\"objects\"]:\n",
    "                output[image][\"objects\"][class_name] = {}\n",
    "            curr_obj_index = len(output[image][\"objects\"][class_name].keys())\n",
    "            output[image][\"objects\"][class_name][curr_obj_index] = {}\n",
    "            output[image][\"objects\"][class_name][curr_obj_index][\"size\"] = obj_area\n",
    "            output[image][\"objects\"][class_name][curr_obj_index][\"coordinates\"] = [obj_x1, obj_y1, obj_x2, obj_y2]\n",
    "            #############################################################\n",
    "        curr_objects = list(dict.fromkeys(curr_objects))\n",
    "\n",
    "        output[image][\"scenes\"] = {}\n",
    "        for scene in scenes:\n",
    "            # print (\"checking scene '{:}'\".format(scene))\n",
    "            output[image][\"scenes\"][scene] = {}\n",
    "            output[image][\"scenes\"][scene][\"ans\"] = not True\n",
    "            output[image][\"scenes\"][scene][\"count\"] = 0\n",
    "            for obj in curr_objects:\n",
    "            # print (\"- checking object '{:}'\".format(obj))\n",
    "                if (obj in scenes[scene]):\n",
    "                    output[image][\"scenes\"][scene][\"ans\"] = True\n",
    "                    output[image][\"scenes\"][scene][\"count\"] += 1\n",
    "    # flow 3 complete\n",
    "    depth_predictions = depth_estimator(curr_ig)\n",
    "    for obj in output[image][\"objects\"]:\n",
    "        for obj_i in output[image][\"objects\"][obj]:\n",
    "            # (to be added here) build logic for depth estimation from the matrix 'depth_predictions'\n",
    "            output[image][\"objects\"][obj][obj_i][\"depth\"] = 10 # static, for now\n",
    "\n",
    "    curr_output = output[image]\n",
    "\n",
    "    # flow 4 complete\n",
    "    sorted_objects = []\n",
    "    size_weight = 0.01\n",
    "    depth_weight = 0.01\n",
    "    # cls = class\n",
    "    for cls in curr_output['objects']:\n",
    "        # ob = object\n",
    "        for index, ob in enumerate(curr_output['objects'][cls]):\n",
    "            coords = curr_output['objects'][cls][ob]['coordinates']\n",
    "            # The score needs to be formulated\n",
    "            score = \\\n",
    "                size_weight * curr_output['objects'][cls][ob]['size'] + \\\n",
    "                depth_weight * curr_output['objects'][cls][ob]['depth']\n",
    "            if score > score_threshold:\n",
    "                sorted_objects.append([cls, index, score, coords])\n",
    "\n",
    "\n",
    "    sorted_objects = sorted(sorted_objects, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "\n",
    "    ori_imagePath = os.path.join(original_folder, str(image) + '.jpg')\n",
    "    dpt_imagePath = os.path.join(depth_folder, str(image) + '.jpg')\n",
    "    txt_filePath = os.path.join(txt_folder, str(image) + '.txt')\n",
    "\n",
    "    # Save original image (flow 5 complete)\n",
    "    curr_ig.save(ori_imagePath)\n",
    "\n",
    "    # Save depth estimation result (flow 6 complete)\n",
    "    depth_predictions['depth'].save(fp=dpt_imagePath)\n",
    "\n",
    "    # Save coordinates (flow 7 complete)\n",
    "\n",
    "    len_SO = len(sorted_objects)\n",
    "    with open(txt_filePath, 'w') as f:\n",
    "        if len_SO >= 5:\n",
    "            for o in sorted_objects:\n",
    "                f.write(f'{o[3][0]},{o[3][1]},{o[3][2]},{o[3][3]}\\n')\n",
    "        else:\n",
    "            # If less than 5 objects are detected\n",
    "            for o in sorted_objects:\n",
    "                f.write(f'{o[3][0]},{o[3][1]},{o[3][2]},{o[3][3]}\\n')\n",
    "            for _ in range(5-len_SO):\n",
    "                f.write('0,0,0,0\\n')\n",
    "\n",
    "    # Write on csv file (flow 8 complete)\n",
    "    csv_content = [\n",
    "        # index, oriPath, dptPath, txtPath, label\n",
    "        image, ori_imagePath, dpt_imagePath, txt_filePath, 0\n",
    "    ]\n",
    "\n",
    "    with open(csv_file, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(csv_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k--FvGDBzz3V"
   },
   "source": [
    "# **Xtras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8tgWehEz6hc"
   },
   "source": [
    "## **1) Dinov2 (it tracks objects in the video)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwUTVDdnXFRc"
   },
   "outputs": [],
   "source": [
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOCFGMLyXGtX"
   },
   "outputs": [],
   "source": [
    "dinov2_vits14.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZYqDcn83Oo2"
   },
   "source": [
    "## **2) Topic (one-liner description)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvEuRPtFz9vz"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "t8tgWehEz6hc",
    "_ZYqDcn83Oo2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "039c0b168f1a46fb91674d56bf8048d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1de0adc0702142bf89e236365f91a84b",
       "IPY_MODEL_ccfe9f7cdd144c84a24ee465fad7e61c",
       "IPY_MODEL_13c9cbab9f7d4be5978be65a98df8f17"
      ],
      "layout": "IPY_MODEL_f7b3c7a84f504d9e9869131e2983b354"
     }
    },
    "03b868f0f5244e0c973629869bd0f0ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_940756be252d487b88abd8775c6b7268",
      "placeholder": "",
      "style": "IPY_MODEL_925f946a7ea34e1487b2848f53a2bf74",
      "value": " 245M/245M [00:23&lt;00:00, 23.7MB/s]"
     }
    },
    "13c9cbab9f7d4be5978be65a98df8f17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef783b28b9444ee9931f08a309fe6817",
      "placeholder": "",
      "style": "IPY_MODEL_4555a4b079a8435e99af9ff3297c393f",
      "value": " 920/920 [00:00&lt;00:00, 31.0kB/s]"
     }
    },
    "1c75193723c246a89395a596f9f41b78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa38138784ea4dad8ccfbf63fa772eaf",
       "IPY_MODEL_887294806a7541b9ad1f5eafdc396353",
       "IPY_MODEL_dda27236d3de4dc5ac7e215ba705ec87"
      ],
      "layout": "IPY_MODEL_3afb0994afda44c081136f2a5c7b7d0a"
     }
    },
    "1dc9153abb414b53879c699c1deda5e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1de0adc0702142bf89e236365f91a84b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6bc6c52d04254d26ae0456e8ba2c8d12",
      "placeholder": "",
      "style": "IPY_MODEL_954a7ca6fd3943e688e68c007346f04e",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "3afb0994afda44c081136f2a5c7b7d0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b642cccc8984625b12517535762788e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4555a4b079a8435e99af9ff3297c393f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66691537bd85499a99b71de0b7c5ca98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bc6c52d04254d26ae0456e8ba2c8d12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a2aa0081611464b8fa74ea418bc5c78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "887294806a7541b9ad1f5eafdc396353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db6d1a01c4504af8a5c5938f428c691a",
      "max": 137,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9a595eb90c4489ebf40e2a4effe666a",
      "value": 137
     }
    },
    "8c99f279fbc042b8a585ac26161c2ff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fa2ec06d6c549a887274db3cacf3ae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3a4f74d222147d9bb58cc0e6db2538b",
      "max": 245258793,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a29057d635a4687b9ee922fb2bea5e2",
      "value": 245258793
     }
    },
    "925f946a7ea34e1487b2848f53a2bf74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "940756be252d487b88abd8775c6b7268": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "954a7ca6fd3943e688e68c007346f04e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a29057d635a4687b9ee922fb2bea5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a9a595eb90c4489ebf40e2a4effe666a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b18c7c202c2c44ce94f0a94e8fe963a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3c335b678ef49d1975fa6a9a8674cbe",
      "placeholder": "",
      "style": "IPY_MODEL_8c99f279fbc042b8a585ac26161c2ff0",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "b3c335b678ef49d1975fa6a9a8674cbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccfe9f7cdd144c84a24ee465fad7e61c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a2aa0081611464b8fa74ea418bc5c78",
      "max": 920,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbfe7cb55fd448a3b64ce92b6b1a1d2d",
      "value": 920
     }
    },
    "d3a4f74d222147d9bb58cc0e6db2538b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9e17a653d7e4f58a7ab7ce08e678cbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db6d1a01c4504af8a5c5938f428c691a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dda27236d3de4dc5ac7e215ba705ec87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b642cccc8984625b12517535762788e",
      "placeholder": "",
      "style": "IPY_MODEL_d9e17a653d7e4f58a7ab7ce08e678cbe",
      "value": " 137/137 [00:00&lt;00:00, 8.59kB/s]"
     }
    },
    "ee7e1598db4d4caba3841a627f1ff166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b18c7c202c2c44ce94f0a94e8fe963a9",
       "IPY_MODEL_8fa2ec06d6c549a887274db3cacf3ae4",
       "IPY_MODEL_03b868f0f5244e0c973629869bd0f0ae"
      ],
      "layout": "IPY_MODEL_66691537bd85499a99b71de0b7c5ca98"
     }
    },
    "ef783b28b9444ee9931f08a309fe6817": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f52e102e158845b1abb37cdb2477b3d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7b3c7a84f504d9e9869131e2983b354": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa38138784ea4dad8ccfbf63fa772eaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f52e102e158845b1abb37cdb2477b3d0",
      "placeholder": "",
      "style": "IPY_MODEL_1dc9153abb414b53879c699c1deda5e8",
      "value": "Downloading ()rocessor_config.json: 100%"
     }
    },
    "fbfe7cb55fd448a3b64ce92b6b1a1d2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
